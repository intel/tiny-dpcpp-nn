from tiny_dpcpp_nn import Encoding
import torch
import numpy as np
import math


def test_grid():

    scale = 1
    N_min = 16
    L = 16

    grid_encodings = Encoding(
        n_input_dims=3,
        encoding_config={
            "otype": "HashGrid",
            "type": "Hash",
            "n_levels": 16,
            "n_features_per_level": 2,
            "log2_hashmap_size": 19,
            "base_resolution": N_min,
            "per_level_scale": np.exp(np.log(2048 * scale / N_min) / (L - 1)),
            "interpolation": "Linear",
        },
    )

    input_encoding = torch.ones((10, 3)).to("xpu")
    output_encoding = grid_encodings(input_encoding)

    grid_output = torch.Tensor(
        [
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
            [
                1.1325e-05,
                -4.1187e-05,
                -7.4506e-06,
                -3.8445e-05,
                -5.7161e-05,
                3.1471e-05,
                -6.5804e-05,
                -4.4107e-06,
                -1.9014e-05,
                -2.9027e-05,
                -2.2352e-05,
                1.3053e-05,
                -4.7386e-05,
                -4.6253e-05,
                -2.3186e-05,
                8.4043e-06,
                2.6047e-05,
                5.0247e-05,
                -7.9870e-06,
                -1.0788e-05,
                -6.4373e-06,
                4.4703e-06,
                4.2915e-05,
                1.1921e-07,
                1.1027e-05,
                5.9605e-06,
                -2.9862e-05,
                1.4484e-05,
                4.4107e-06,
                -1.3649e-05,
                -2.5451e-05,
                2.4438e-06,
            ],
        ]
    ).to("xpu")
    assert torch.all(torch.isclose(output_encoding, grid_output, atol=1e-3))


def test_spherical():
    spherical_encodings = Encoding(
        n_input_dims=3,
        encoding_config={
            "otype": "SphericalHarmonics",
            "degree": 4,
        },
    )

    input_encoding = torch.ones((10, 3)).to("xpu")
    output_encoding = spherical_encodings(input_encoding)

    spherical_output = torch.Tensor(
        [
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
            [
                0.2821,
                -0.4886,
                0.4886,
                -0.4886,
                1.0925,
                -1.0925,
                0.6308,
                -1.0925,
                0.0000,
                -1.1801,
                2.8906,
                -1.8282,
                0.7464,
                -1.8282,
                0.0000,
                1.1801,
            ],
        ]
    ).to("xpu")
    assert torch.all(torch.isclose(output_encoding, spherical_output, atol=1e-3))


def test_identity():

    identity_encodings = Encoding(
        n_input_dims=3,
        encoding_config={
            "otype": "Identity",
            "scale": 1.0,
            "offset": 0.0,
        },
    )

    input_encoding = torch.ones((10, 3)).to("xpu")
    output_encoding = identity_encodings(input_encoding)

    assert torch.all(torch.isclose(input_encoding, output_encoding))


def test_padding():

    identity_encodings = Encoding(
        n_input_dims=3,
        encoding_config={
            "otype": "Identity",
            "scale": 1.0,
            "offset": 0.0,
        },
        n_output_dims=5346,
    )

    input_encoding = torch.ones((10, 3)).to("xpu")
    output_encoding = identity_encodings(input_encoding)
    assert identity_encodings.n_output_dims == 5346
    assert output_encoding.shape[1] == 5346


def test_frequency():
    frequency_encoding = Encoding(
        n_input_dims=1,
        encoding_config={
            "otype": "Frequency",
            "n_frequencies": 1.0,
        },
    )
    # Set up input and output
    batch_size = 1
    input_val = 1.234
    input_tensor = torch.ones(batch_size, 1).xpu() * input_val

    # Run forward pass
    output_tensor = frequency_encoding.forward(input_tensor)
    # Expected output values
    expected = torch.tensor(
        [[math.sin(math.pi * input_val), math.cos(math.pi * input_val)]], device="xpu"
    )
    # Check if result matches expected values
    assert torch.allclose(
        output_tensor, expected, atol=1e-4
    ), f"Output: {output_tensor}, Expected: {expected}"


if __name__ == "__main__":
    # test_identity()
    # test_spherical()
    # test_grid()
    # test_frequency()
    test_padding()
